import pandas as pd
from lxml import etree
import csv

from openPM.Process.Instance import Instance
from openPM.Process.Activity import Activity
from openPM.Process.Attribute import Attribute
from os.path import splitext
import timeit  # TODO remove after debugging


# TODO Build an Iterator!
class Process:
    counter = 0

    def __init__(self, path, delimiter: str = ",", case_column: int = 0):
        """
        :param path: Path to the file that sould be parsed.
        :param delimiter: For CSV import - Delimiter of the CSV values.
        :param case_column: For CSV import - Number of the Instance column.
        """
        # TODO-fixme I want to be a singelton!
        self.process = pd.DataFrame(columns=["instance_attributes", "event_attributes"])
        self.instance_id = 1

        _, ext = splitext(path)
        if ext == ".xes":
            self.__parse_xes(path)
        elif ext == ".csv":
            case_index = case_column-1 # Done because user will say it's the 4th column not the index 3
            self.__parse_csv(path, delimiter, case_index)


    def __repr__(self):
        return self.process.to_string()

    def __parse_xes(self, path: str):
        print("STATISTICS:  ", file=open("evaluate_1.txt", "a"))
        start1 = timeit.default_timer()
        for _, trace in etree.iterparse(path, tag='trace'):
            instance = Instance()
            for event in trace.iterchildren("event"):
                act = Activity()
                for attribute in event.iterchildren():
                    pass
                    atr = Attribute(attribute.attrib["key"], attribute.attrib["value"])
                    act.add_attribute(atr)
                instance.add_activity(act)
            self.add_process_instance(instance)
        stop1 = timeit.default_timer()
        print(
            " Time to parse only objects instantiation and act.add_attribute(), "
            "inst.add_activity [s], add_process_instance():  ",
            stop1 - start1, file=open("evaluate_1.txt", "a"))

    def __parse_csv(self, path: str, delimiter: str = ",", case_indicator_column: int = 0):
        """
        :param path: Path as String to the file that should be parsed.
        :param delimiter: Delimiter of the CSV values. Default is ",".
        :param case_indicator_column: Number of the Column which contains the case_id/instance number.
        """
        print("STATISTICS:  ", file=open("evaluate_csv_parser_1.txt", "a"))
        start1 = timeit.default_timer()
        with open(path) as csv_file:
            reader = csv.reader(csv_file) # TODO Delimiter of CSV should always be ","
            last_case_id = -1
            instance = False
            header = []
            for idx, row in enumerate(reader):
                if idx == 0:  # Get the header row
                    header = row
                else:
                    case_id = row[case_indicator_column]
                    if int(case_id) != last_case_id:
                        if instance:
                            self.add_process_instance(instance)
                        instance = Instance()
                        last_case_id = int(case_id)
                    act = Activity()
                    for j, item in enumerate(row):
                        attr = Attribute(key=header[j], value=item)
                        act.add_attribute(attr)
                    instance.add_activity(act)
        stop1 = timeit.default_timer()
        print(
            " Time to parse csv: ",
            stop1 - start1, file=open("evaluate_csv_parser_1.txt", "a"))


    def __iter__(self):
        return self

    # TODO-fixme it seems that the iterator doesn't consider order of isntances in event log
    # TODO-note Using the iterator is now faster, but still not the fastest way to do it because of the df.append()function.
    def __next__(self):
        process = self.process
        index_array = process.index.values
        max_index = max(index_array)
        for _ in index_array:
            if self.counter >= max_index:
                self.counter = 0
                raise StopIteration
            else:
                self.counter += 1
                instance = self.process.iloc[self.counter]
                events = instance["event_attributes"]

                instance_dicts = []
                for event in events:
                    attributes = event.get_attributes()
                    dicts = {}
                    for attribute in attributes:
                        dicts[attribute.get_key()] = attribute.get_value()
                    instance_dicts.append(dicts)

                df = pd.DataFrame(instance_dicts)
                return df

    def add_process_instance(self, instance: Instance):
        attributes = instance.get_attributes()

        events = instance.get_activities()

        index = len(self.process)  # last index of self.process
        # TODO make notes!
        self.process.at[index, "instance_attributes"] = attributes
        self.process.at[index, "event_attributes"] = events

    # TODO-fixme Needs to be rewriten because datastructure changed!
    # TODO Type hinting pandas.series?
    def get_all_activities(self):
        activities = self.process.loc[:, "event_attributes"]  # returns series. Unique kills duplicates
        return activities

    def get_start_activities(self, activity_column: str) -> set:
        start_activities = set()
        for instance in self:
            start_activities.add(instance.loc[0, activity_column])
        return start_activities

    def get_end_activities(self, activity_column: str):
        end_activities = set()
        for instance in self:
            end_activities.add(instance.iloc[-1][activity_column])
        return end_activities

    def get_process(self):
        return self.process

    @staticmethod
    def flat_process(process):
        instances = [instance for instance in process]
        df = pd.concat([instance for instance in instances], ignore_index=True)
        return df
